{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GlrDP832IOsZ"
   },
   "source": [
    "### I. All the necessary imports required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 942,
     "status": "ok",
     "timestamp": 1599777890719,
     "user": {
      "displayName": "Nikunj Rajesh Kotecha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giz2eVWmlNh1zjP20AbaTxZfckB5rNRoxmclty_=s64",
      "userId": "16556735897646851289"
     },
     "user_tz": 240
    },
    "id": "crt7LWmcAs5I"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob, os\n",
    "import numpy as np\n",
    "import csv\n",
    "import pdb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CsoeCiXiIJnz"
   },
   "source": [
    "### Extract and dump all the features for each of the modality\n",
    "**Note: please change the two paths according to your folder location on drive**\n",
    "\n",
    "\n",
    "*   data = < path to existing data folder where all folder for each modality are kept >\n",
    "*   out = < path to folder where all the features is to be dumped >\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 82
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7265,
     "status": "ok",
     "timestamp": 1599777905357,
     "user": {
      "displayName": "Nikunj Rajesh Kotecha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giz2eVWmlNh1zjP20AbaTxZfckB5rNRoxmclty_=s64",
      "userId": "16556735897646851289"
     },
     "user_tz": 240
    },
    "id": "6NhvqPYhtAwu",
    "outputId": "de26799e-6505-4816-bc84-75165cf8e2dc"
   },
   "outputs": [],
   "source": [
    "data = 'irpev_clean'\n",
    "out = 'hw01'\n",
    "\n",
    "def get_statistics( arr ):\n",
    "#   '''\n",
    "#   Write a function to gather the statistics for a single participant for a particular modality\n",
    "\n",
    "#   Input:\n",
    "#   arr -> numpy array\n",
    "\n",
    "#   Return:\n",
    "#   mean, max, min, std -> all features of the respective columns in the csv\n",
    "#   '''\n",
    "    arr = arr[1:]\n",
    "    mean = np.mean(arr,axis = 0)\n",
    "    std = np.std(arr, axis = 0)\n",
    "    mmax = np.max(arr,axis = 0)\n",
    "    mmin = np.min(arr,axis=0)\n",
    "    return mean, std, mmax, mmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def judge(array):\n",
    "\n",
    "    if array[0] == 'A':\n",
    "        array = ['1']+ array\n",
    "    elif array[0] == 'D':\n",
    "        array = ['0'] + array\n",
    "    return array\n",
    "\n",
    "def extract_features( data, dest, folder, features, common ):\n",
    "#   '''\n",
    "#   Write a function to generate all the features for each entry for each modality\n",
    "\n",
    "#   Input:\n",
    "#   data = path to folder where all modality folders are present\n",
    "#   dest = path to dump the modality features\n",
    "#   folder = modality name [ vid, gsr, ego ]\n",
    "#   features = feature columns names to give header for csv\n",
    "#   common = common column names to give header for csv\n",
    "#   '''\n",
    "    cols = common + features\n",
    "    with open( dest, 'w' ) as f:\n",
    "        writer = csv.writer( f )\n",
    "        writer.writerow( cols )\n",
    "        row = []\n",
    "        for filepath in glob.glob( f'{data}/{folder}/*.csv' ):\n",
    "            root_split = os.path.splitext(filepath)[0]                 #i\n",
    "            ii_split = root_split.split('_')[1:]                       #ii \n",
    "            ii_split[0] = ii_split[0][-1]\n",
    "            ii_split = judge(ii_split)\n",
    "            read_eachfile = np.genfromtxt(filepath,delimiter=',')      #iii\n",
    "            re = pd.DataFrame(read_eachfile)\n",
    "            re.dropna(how = 'all')\n",
    "            mean, std,mmax, mmin = get_statistics(re)                  #iv\n",
    "            row = np.r_[ii_split,mean,std,mmax,mmin]\n",
    "\n",
    "            # for each entry in the modality, extract the features and dump into the csv\n",
    "            # i) Extract the filename to prefix and extension (command splitext might be useful here)\n",
    "            # ii) Split the prefix at '_' to find out the label 'A' or 'D'; set to 1 and 0\n",
    "            # iii) Extract the data into an array from the csv file (numpy command genfromtext might be useful here )\n",
    "            # iv) Call the above function get_statistics( arr ) to compute your statistics for this\n",
    "            #use the command below to write the newly formed feature row into the new file using writer.writerow\n",
    "            writer.writerow( row )\n",
    "    print( f'Done with {folder} features' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a dictonary for each modality and the respective feature column names\n",
    "# Modify it to match your own feature names (from the statistics you calculated)\n",
    "# Note that I'm only using mean and std deviation, you should add max and min as well\n",
    "d = {\n",
    "    'vid': [ 'engagement_mean', 'contempt_mean', 'engagement_std', 'contempt_std', 'engagement_max', 'contempt_max', 'engagement_min', 'contempt_min' ],\n",
    "    'gsr': [ 'cv_mean', 'cv_std','cv_max', 'cv_min' ],\n",
    "    'ego': [ 'delta_x_mean', 'delta_y_mean', 'delta_angle_mean', 'delta_x_std', 'delta_y_std', 'delta_angle_std','delta_x_max', 'delta_y_max', 'delta_angle_max','delta_x_min', 'delta_y_min', 'delta_angle_min' ]\n",
    "}\n",
    "# common column names\n",
    "common = ['label', 'cls', 'pair', 'participant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with vid features\n",
      "Done with gsr features\n",
      "Done with ego features\n",
      "Done extracting features for each modality.\n"
     ]
    }
   ],
   "source": [
    "# Collect all the features from each modality and merge them; use the pandas dataframe here\n",
    "res = pd.DataFrame()\n",
    "i_h = 0\n",
    "for folder, features in d.items():\n",
    "    # i) create the destination folder\n",
    "    dest = f'{out}/{folder}.csv'\n",
    "    \n",
    "    # ii) call extract_features( data, dest, folder, features, common )\n",
    "    extract_features( data, dest, folder, features, common )\n",
    "    temp = pd.read_csv(dest)\n",
    "    \n",
    "    #frist time to add data in 'res'\n",
    "    if i_h == 0:\n",
    "        res = temp\n",
    "        i_h += 2\n",
    "        continue\n",
    "        \n",
    "    # iii) merge the features for each modality to create one aggregate feature\n",
    "    res = pd.merge(res,temp,on= ['label', 'cls', 'pair', 'participant'],how='left')\n",
    "    \n",
    "print( 'Done extracting features for each modality.' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now review your dataset consisting of {X, y}, then split it into training and test set with 9:1 ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 383,
     "status": "ok",
     "timestamp": 1599777941115,
     "user": {
      "displayName": "Nikunj Rajesh Kotecha",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giz2eVWmlNh1zjP20AbaTxZfckB5rNRoxmclty_=s64",
      "userId": "16556735897646851289"
     },
     "user_tz": 240
    },
    "id": "bJ6HgD2ADUhz",
    "outputId": "82ee9681-c887-452c-b7cd-ce0a2c692e2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of my feature matrix:  (48, 28)\n",
      "Size -> X_train: (43, 24), X_test: (5, 24), y_train: (43,), y_test: (5,)\n"
     ]
    }
   ],
   "source": [
    "# This code splits your data [X,y] into 90% training and 10%test.\n",
    "print('Shape of my feature matrix: ',res.shape)\n",
    "\n",
    "features = res.columns[ ~res.columns.isin( common ) ]\n",
    "X = res[ features ]\n",
    "y = res[ 'label' ]\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.1, random_state=3 )\n",
    "print( f'Size -> X_train: {X_train.shape}, X_test: {X_test.shape}, y_train: {y_train.shape}, y_test: {y_test.shape}' )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the actual classifiers and print your performance metric(s), and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f5anImVRMbkw",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandForClassif 10-fold cross val ---> average:0.6900000000000001, std:0.19078784028338913\n",
      "random forest accuracy:  1.0\n",
      "Rf confusion matrix:\n",
      "[[3 0]\n",
      " [0 2]]\n",
      "====================================================================================\n",
      "SVM 10-fold cross val ---> average:0.63, std:0.10535653752852739\n",
      "random forest accuracy:  0.6\n",
      "SVM confusion matrix:\n",
      "[[3 0]\n",
      " [2 0]]\n",
      "====================================================================================\n",
      "NNC 10-fold cross val ---> average:0.45999999999999996, std:0.22671568097509268\n",
      "random forest accuracy:  0.6\n",
      "Rf confusion matrix:\n",
      "[[1 2]\n",
      " [0 2]]\n",
      "====================================================================================\n",
      "====================================================================================\n",
      "GBDT\n",
      "GBDT 10-fold cross val ---> average:0.79, std:0.07348469228349534\n",
      "random forest accuracy:  0.6\n",
      "confusion matrix: \n",
      "[[3 0]\n",
      " [1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "res_mean = {'randomForest': -1,\n",
    "           'SVM':-1,\n",
    "           'NeuralNet':-1}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# i) Run Random Forest, report results\n",
    "rf = RandomForestClassifier(n_estimators=4, criterion='gini',max_features=5)\n",
    "rf.fit(X_train,y_train)\n",
    "rf_predict = rf.predict(X_test)                                           #predict the test set\n",
    "rf_accuracy = accuracy_score(y_test,rf_predict)                           # get accuracy of this model\n",
    "cro_ = cross_val_score(rf, X_train, y_train, scoring='accuracy', cv=10)   #10 fold cross validation, same as below other models\n",
    "print(f'RandForClassif 10-fold cross val ---> average:{cro_.mean()}, std:{cro_.std()}' )\n",
    "print('random forest accuracy: ',rf_accuracy)\n",
    "print('Rf confusion matrix:')\n",
    "print(confusion_matrix(y_test,rf_predict))\n",
    "res_mean['randomForest'] = cro_.mean()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ii) Run SVM (remember there can be multiple kernels, just one is fine), report results\n",
    "print('====================================================================================')\n",
    "svm = SVC(probability=True,random_state=10)\n",
    "svm.fit(X_train,y_train)\n",
    "svm_predict = svm.predict(X_test)\n",
    "svm_accuracy = accuracy_score(y_test,svm_predict)\n",
    "cro_svm = cross_val_score(svm, X_train, y_train, scoring='accuracy', cv=10)\n",
    "print(f'SVM 10-fold cross val ---> average:{cro_svm.mean()}, std:{cro_svm.std()}' )\n",
    "print('random forest accuracy: ',svm_accuracy)\n",
    "print('SVM confusion matrix:')\n",
    "print(confusion_matrix(y_test,svm_predict))\n",
    "res_mean['SVM'] = cro_svm.mean()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# iii) Run a Neural Network classifier, report results\n",
    "print('====================================================================================')\n",
    "nnc = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5,), random_state=0)\n",
    "nnc.fit(X_train,y_train)\n",
    "nnc_predict = nnc.predict(X_test)\n",
    "nnc_accuracy = accuracy_score(y_test,nnc_predict)\n",
    "cro_nnc = cross_val_score(nnc, X_train, y_train, scoring='accuracy', cv=10)\n",
    "print(f'NNC 10-fold cross val ---> average:{cro_nnc.mean()}, std:{cro_nnc.std()}' )\n",
    "print('random forest accuracy: ',nnc_accuracy)\n",
    "print('Rf confusion matrix:')\n",
    "print(confusion_matrix(y_test,nnc_predict))\n",
    "res_mean['NeuralNet'] = cro_nnc.mean()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('====================================================================================')\n",
    "# print(res_mean)\n",
    "# iv) BONUS: Run and report on a fourth classifier of your choice that performs better than the 3 above\n",
    "# print('====================================================================================')\n",
    "# print('The best classifier is'+ 'RandomForestClassifier')\n",
    "# rf_confus = confusion_matrix(y_test,rf_predict)                                           #get confusion matrix\n",
    "# print('RandomForestClassifier confusion matrix: ')\n",
    "# print(rf_confus)\n",
    "\n",
    "print('====================================================================================')\n",
    "print('GBDT')\n",
    "#GBDT\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "dt = GradientBoostingClassifier(random_state = 29)\n",
    "dt = dt.fit(X_train,y_train)\n",
    "dt_pre = dt.predict(X_test)\n",
    "dt_accuracy = accuracy_score(y_test,dt_pre)\n",
    "dt_cro = cross_val_score(dt, X_train, y_train, scoring='accuracy', cv=10)\n",
    "print(f'GBDT 10-fold cross val ---> average:{dt_cro.mean()}, std:{dt_cro.std()}' )\n",
    "print('random forest accuracy: ',nnc_accuracy)\n",
    "dt_confus = confusion_matrix(y_test,dt_pre)                                                #get confusion matrix\n",
    "print('confusion matrix: ')\n",
    "print(dt_confus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNFt0puQ4JCxKHcy/BAOuLi",
   "collapsed_sections": [],
   "name": "Agreement/Disagreement.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
